{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eeffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91966\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\91966\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\91966\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91966\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\91966\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: html5lib in c:\\users\\91966\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\91966\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\91966\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: pyphen in c:\\users\\91966\\anaconda3\\lib\\site-packages (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import ast\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "!pip install requests\n",
    "!pip install html5lib\n",
    "!pip install bs4\n",
    "!pip install pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a48d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(query):\n",
    "    url = f'https://www.google.com/search?q={query}&tbm=nws'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if 'url?q=' in a['href']]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f866287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/url?q=https://www.abplive.com/news/india/pm-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/url?q=https://navbharattimes.indiatimes.com/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/url?q=https://timesofindia.indiatimes.com/ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/url?q=https://www.hindustantimes.com/india-ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/url?q=https://indianexpress.com/article/citie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/url?q=https://economictimes.indiatimes.com/ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/url?q=https://www.thehindu.com/news/national/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/url?q=https://www.livemint.com/news/india/pm-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/url?q=https://www.ndtv.com/india-news/congres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/url?q=https://indianexpress.com/article/polit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/url?q=https://www.indiatoday.in/india-today-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/url?q=https://www.thehindu.com/news/national/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/url?q=https://www.rediff.com/news/report/pix-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/url?q=https://recipes.timesofindia.com/web-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/url?q=https://www.tribuneindia.com/news/india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/url?q=https://www.timesnownews.com/india/pm-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/url?q=https://support.google.com/websearch%3F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/url?q=https://accounts.google.com/ServiceLogi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   /url?q=https://www.abplive.com/news/india/pm-n...\n",
       "1   /url?q=https://navbharattimes.indiatimes.com/i...\n",
       "2   /url?q=https://timesofindia.indiatimes.com/ind...\n",
       "3   /url?q=https://www.hindustantimes.com/india-ne...\n",
       "4   /url?q=https://indianexpress.com/article/citie...\n",
       "5   /url?q=https://economictimes.indiatimes.com/ne...\n",
       "6   /url?q=https://www.thehindu.com/news/national/...\n",
       "7   /url?q=https://www.livemint.com/news/india/pm-...\n",
       "8   /url?q=https://www.ndtv.com/india-news/congres...\n",
       "9   /url?q=https://indianexpress.com/article/polit...\n",
       "10  /url?q=https://www.indiatoday.in/india-today-i...\n",
       "11  /url?q=https://www.thehindu.com/news/national/...\n",
       "12  /url?q=https://www.rediff.com/news/report/pix-...\n",
       "13  /url?q=https://recipes.timesofindia.com/web-st...\n",
       "14  /url?q=https://www.tribuneindia.com/news/india...\n",
       "15  /url?q=https://www.timesnownews.com/india/pm-n...\n",
       "16  /url?q=https://support.google.com/websearch%3F...\n",
       "17  /url?q=https://accounts.google.com/ServiceLogi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query  = 'modi'\n",
    "# query = input(\"Enter your query: \")\n",
    "links = get_news_links(query)\n",
    "\n",
    "for link in links:\n",
    "    link = link[7:]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(links)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(df) \n",
    "for i in range(0, n):\n",
    "  url = df.iloc[i,0]\n",
    "\n",
    "#getting website url\n",
    "  r = requests.get(url)\n",
    "  htmlContent = r.content\n",
    "\n",
    "#parse the artcile\n",
    "  soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "\n",
    "\n",
    "#getting the title\n",
    "  title = soup.title\n",
    "\n",
    "#get all paragraphs from the page\n",
    "  paras = soup.find_all('p')\n",
    "\n",
    "#getting the class of the paragraphs\n",
    "  mydivs = soup.find(\"div\", {\"class\": \"td-post-content\"})\n",
    "\n",
    "#finding all the children and getting contents\n",
    "  # children = mydivs.findChildren(\"p\" , recursive=False)\n",
    "  # for child in children:\n",
    "  #   skip = False\n",
    "  #   for c in child.children:\n",
    "  #     if c.name == \"strong\":\n",
    "  #       skip = True\n",
    "  #       break\n",
    "\n",
    "  #   if skip == False:\n",
    "  #     Text = child.string\n",
    "\n",
    "  with open('URL_ID.txt', 'w') as f:\n",
    "    for paragraph in paras:\n",
    "      f.write(paragraph.text + '\\n')\n",
    "\n",
    "  h = open('URL_ID.txt', \"r\")\n",
    "  Text = h.read()\n",
    "  h.close()\n",
    "\n",
    "#writing Paragraph texts in URL_ID file as instructed\n",
    "  # file1 = open(\"URL_ID.txt\", \"w\")\n",
    "  # file1.write(Text)\n",
    "  # file1.close()\n",
    "\n",
    "\n",
    "#getting positive words\n",
    "  f1 = open('positive-words.txt', \"r\")\n",
    "  positive_words = f1.read()\n",
    "  f1.close()\n",
    "\n",
    "\n",
    "\n",
    "#getting negatice words\n",
    "  import chardet\n",
    "  #introducing chardet function to get the type of character used in negative words file\n",
    "  with open(\"negative-words.txt\", 'rb') as file:\n",
    "    raw_data = file.read()\n",
    "    detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "  with open('negative-words.txt', 'r', encoding=detected_encoding) as file:\n",
    "    negative_words = file.read()\n",
    "\n",
    "#making a string for stopwords in a master file\n",
    "#Stopwords divided into two subgroups as 1-Generic, GenericLong, Auditor and 2-Geographic, Currencies, DateandNumber, Names\n",
    "  stop_words = \" \"\n",
    "  filepath = [\"Stopwords\\StopWords_Generic.txt\", 'StopWords/StopWords_GenericLong.txt', 'StopWords/StopWords_Auditor.txt']\n",
    "\n",
    "#for group 1\n",
    "  for file in filepath:\n",
    "    with open(file, 'rb') as f:\n",
    "      raw_data1 = f.read()\n",
    "      detected_encoding = chardet.detect(raw_data1)['encoding']\n",
    "\n",
    "    with open(file, 'r', encoding = detected_encoding) as f:\n",
    "      temp = f.read()\n",
    "      stop_words += temp\n",
    "\n",
    "\n",
    "#for group 2\n",
    "  filepath2 = [\"Stopwords/StopWords_Currencies.txt\", \"Stopwords/StopWords_DatesandNumbers.txt\", \"Stopwords/StopWords_Geographic.txt\", 'Stopwords/StopWords_Names.txt']\n",
    "  for files in filepath2:\n",
    "    with open(files, 'rb') as f2:\n",
    "      raw_data2 = f2.read()\n",
    "      detected_encoding = chardet.detect(raw_data2)['encoding']\n",
    "\n",
    "    with open(files, 'r', encoding = detected_encoding) as f2:\n",
    "      while True:\n",
    "        temp2 = f2.readline()\n",
    "        keyword = temp2.split(\" \")[0]\n",
    "        if temp2.find(\"|\"):\n",
    "          stop_words += keyword + \"\\n\"\n",
    "        else: stop_words += keyword\n",
    "\n",
    "        if not temp2: break\n",
    "  stop_words = os.linesep.join([s for s in stop_words.splitlines() if s])\n",
    "\n",
    "#Task 1.1 => Removing given stopwords from the article\n",
    "#Task 1.2 => Creating a master dictionary of positive and negative words\n",
    "  for keyword in stop_words:\n",
    "    Final = Text.replace(keyword.strip(), '')\n",
    "    positive_words1 = positive_words.replace(keyword.strip(), '')\n",
    "    negative_words1 = negative_words.replace(keyword.strip(), '')\n",
    "\n",
    "  Text_words = Text.split()\n",
    "  stop_words = stop_words.split()\n",
    "  positive_words = positive_words.split()\n",
    "  negative_words = negative_words.split()\n",
    "\n",
    "  df.iloc[i,11] = len(Text_words)\n",
    "#Task 1.3: Getting Positive Words counts and Negative words count and Polarity-Subjectivity Score\n",
    "  import re\n",
    "  positive_count =0\n",
    "  negative_count =0\n",
    "  for word in Text_words:\n",
    "          pattern = r\"\\b(\" + \"|\".join(map(re.escape,positive_words)) + r\")\\b\"\n",
    "          if re.search(pattern, word, re.IGNORECASE):\n",
    "            positive_count += 1\n",
    "\n",
    "  df.iloc[i,2] = positive_count\n",
    "\n",
    "  for modif in Text_words:\n",
    "          pattern = r\"\\b(\" + \"|\".join(map(re.escape,negative_words)) + r\")\\b\"\n",
    "          if re.search(pattern, modif, re.IGNORECASE):\n",
    "            negative_count += 1\n",
    "\n",
    "  df.iloc[i,3] = negative_count\n",
    "\n",
    "  den = (positive_count + negative_count) + 0.000001\n",
    "  polarity_score = (positive_count - negative_count) / den\n",
    "\n",
    "  df.iloc[i,4] = polarity_score\n",
    "\n",
    "#Task 2: Ananlysis of Readability: Getting Total number of words and number of sentences\n",
    "  Total_words = len(Text_words)\n",
    "\n",
    "  import nltk.data\n",
    "  import nltk.tokenize\n",
    "  tokenized_sentences=nltk.sent_tokenize(Text)\n",
    "  number_of_sentences = len(tokenized_sentences)\n",
    "\n",
    "  avg_sentence_length = Total_words/ number_of_sentences\n",
    "  df.iloc[i,10]= avg_sentence_length\n",
    "\n",
    "#Task 4: Counting the number of complex words\n",
    "  import nltk\n",
    "  from nltk.tokenize import word_tokenize\n",
    "  import pyphen\n",
    "  nltk.download('punkt')\n",
    "\n",
    "  dic = pyphen.Pyphen(lang='en') # Load the English hyphenation dictionary\n",
    "  words = word_tokenize(Text) # Tokenize the text into words\n",
    "\n",
    "  syllable_counts = {}\n",
    "  for word in words:\n",
    "      syllable_counts[word] = len(dic.inserted(word).split('-')) # Count the syllables in each word and create a dictionary to store the results\n",
    "\n",
    "  number_of_complex =0\n",
    "  total_syllable =0\n",
    "  for word, count in syllable_counts.items():\n",
    "      total_syllable += count  # TASK 6: Getting average syllable count\n",
    "      if count > 2: number_of_complex += 1 # Find the syllable count for each word\n",
    "\n",
    "  Syllable_per_word = total_syllable/len(Text_words)\n",
    "  df.iloc[i,12] = Syllable_per_word\n",
    "  df.iloc[i,10]= number_of_complex\n",
    "\n",
    "  percentage_of_complex = number_of_complex / Total_words #getting things from TASK 2\n",
    "  df.iloc[i,7]= percentage_of_complex\n",
    "\n",
    "  fog_index = 0.4 * (avg_sentence_length + percentage_of_complex)\n",
    "  df.iloc[i,8]= fog_index\n",
    "\n",
    "# Task 5: Cleaning words using nltk and removing punctuations\n",
    "  nltk.download('stopwords')\n",
    "  from nltk.corpus import stopwords\n",
    "  from nltk.stem.porter import PorterStemmer\n",
    "  ps = PorterStemmer()\n",
    "  corpus = []\n",
    "\n",
    "\n",
    "  clean_words = re.sub('[^a-zA-Z]', ' ', Text)\n",
    "  clean_words = clean_words.lower()\n",
    "  clean_words = clean_words.split()  #removing the punctuations\n",
    "\n",
    "\n",
    "  clean_words = [word for word in clean_words if not word in stopwords.words('english')]\n",
    "  clean_words = ' '.join(clean_words)\n",
    "  corpus.append(clean_words)  #removing stopwords\n",
    "\n",
    "  clean_words = list(clean_words.split(\" \")) #converting clean_words again into list of words to ease the counting\n",
    "  total_clean_words = len(clean_words)\n",
    "\n",
    "  den2 = (len(clean_words) + 0.000001)\n",
    "  subjectivity_score = (positive_count + negative_count)/ den2 #getting subjectivity score from TASK 1\n",
    "  df.iloc[i,5]= subjectivity_score\n",
    "\n",
    "#TASK 7: Getting personal pronouns\n",
    "  pronouns =['I','we' ,'my','ours','us', 'can']\n",
    "  pronouns_count =0\n",
    "\n",
    "  for im in Text_words:\n",
    "    if im in pronouns: pronouns_count +=1\n",
    "\n",
    "  df.iloc[i,13] = pronouns_count\n",
    "\n",
    "# Task 8: Getting average word length\n",
    "  temp0 = re.sub('[^a-zA-Z]', '', Text)\n",
    "  temp0 = temp0.lower()\n",
    "\n",
    "  lst =[]\n",
    "  lst.extend(temp0)\n",
    "  total_charcter_count = len(lst)\n",
    "\n",
    "  avg_word_length = total_charcter_count/ Total_words\n",
    "  df.iloc[i, 14] = avg_word_length\n",
    "  df.iloc[i, 6] = avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_task(companies):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(companies.corr(), ax = ax )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    \n",
    "    st.title('Sigma_task')\n",
    "    \n",
    "    heatmap = sigma_task(companies)\n",
    "    \n",
    "    # Save the plot as an image file\n",
    "    if st.button('Heatmap'):\n",
    "        buffer = io.BytesIO()\n",
    "        \n",
    "        heatmap.savefig(buffer, format= \"png\")\n",
    "        buffer.seek(0)\n",
    "    \n",
    "       # Display the plot as an image in Streamlit\n",
    "        st.image(buffer, use_column_width=True)\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
